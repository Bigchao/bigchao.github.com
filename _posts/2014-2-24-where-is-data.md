---
layout: post
categories: datamining
title: 数据都去哪儿了
---

在开始介绍数据分析之前，我们要了解为什么要数据分析。这就是通常说的原点思想。

以学术界的经济学为例，超一流是搞思想的，用的平实精炼的语言总结经济发展规律。一流的人搞模型，用建模的数学方法来诠释经济发展规律。低一流的人用实证，也就是通常的计量经济学模型来验证。

这样的解释其实不算严谨。这里虽然说低一流，但是其涉及的经济学知识，数据推导也是非常复杂。总结下来就是通常在计量经济学是为了验证理论。

在业界就更加直观，首先有个总体目标，其次是有个技术目标。商业的核心就是创造价值，创造利润。技术在不断迭代，而主旋律似乎从来没有变过。所以技术是为了真正的商业目的而服务的。这里稍微跑了题，所以对于大多数技术团队，需要和其他的团队进行配合为了整体用户的体验而努力，不可太过纠结于技术。

回到数据分析，刚介绍了数据分析的目的。 

下面简单介绍下数据分析的基本步骤。

最最重要的是，对数据的整体把握和感觉。这个有点玄虚的感觉，但确实如此。我们可以理解为经过时间磨砺的高手在看到数据的时候往往可以知道这个数据感觉是对的还是不对的，如果不对哪里不对。我了解到那些资深的首席经济分析师，精算师通常可以有这种感觉。当然，我们在初期的时候容易拘泥于技术，如何把领导交给的任务完成好，但是这种感觉的培养是必不可少的。比如，我是经济学出身，从业于商业智能，技术还可以。有时候我的同学或者同事请我帮我处理分析一些数据，其实这些基本步骤大抵差不多。就算当我得到一些结果的时候我不会解读，不知道是不是哪儿出问题了。所以，我们在处理自己行业数据的时候应避免出现这样的问题：得到了结果不会解读。

数据挖掘和数据仓库的大牛Herb Edelstein说过这样一句话:

>“If you’ve got terabytes of data, and you’re relying on data mining to find interesting things in 
there for you, you’ve lost before you’ve even begun. You really need people who understand what 
it is they are looking for – and what they can do with it once they find it.” 

大意是说，如果你有TB量级的数据，你想依靠数据挖掘来找到你想要的有意思的东西，你在开始前就已经迷失了。

这个是我希望大家可以时刻记住的。

其次，数据分析都是从最基础最底层的描述性分析开始。

这个很简单，这个很重要。

了解大概的数据分布情况，基本特征，对于继续分析是很有必要的。在这个基础上再进行更加深化的分析。

以淘宝为例，我现在想研究双十一促销对商家和淘宝平台的影响。应该怎么办？

起码先把11月每天的销量统计一遍，肯定不难发现11月11日销量特别高。然后分地域，分商家，分淘宝和天猫再做一遍。这相当于比较高层级的。

这个时候会看出数据的基本趋势，再选取平均值作为基准，按比例进行同样的运算。这样一点点进行深入分析。

我发现增长了或者下降了，再有针对性的深入分析。一步步都有条理，很严谨。

再次，就是进行回归分析，基于机器学习的预测等等稍微高难一点的。

总结下来：

1. 数据的把握
2. 统计描述性分析
3. 进一步深度分析

大概是这样。

下面进入动手阶段。

我提到过，这个专栏会讲到用R，但是不会从头来讲。所以这里我会给出一些推荐教材和推荐文章。

1. 有关国外的，还是最推荐[R-blogger](http://www.r-bloggers.com/),有关业界学界用R的最新动态，可以说内容丰富。每次看过之后都觉得很通透很爽快。通过这个网站自然可以发掘到其他很多牛人的博客。点进去看就可以了。
2. [Quick-R](http://www.statmethods.net/) 这个特别推荐。尤其是初学者，可以把整个网站都过一遍，干活非常的多。
3. [UCLA的idre](http://www.ats.ucla.edu/stat/r/)
1. 肖凯老师的博客可以说是一个启蒙非常好的地方。[http://xccds1977.blogspot.hk](http://xccds1977.blogspot.hk)。但是据说这个在国内被墙掉了，不知道为什么。有心人需要想想办法了。
2. 关于书籍。我看过讲R的书多多少少也有10多本了，对于初学者我还是最推荐《The Art of R Programming》，思路相当清晰，我觉得比《R in Action》，《R in nutshell》都要好。所以大家可以搜一下这本书。其中可以不看讲多线程的部分，前边的内容一点一点跟着来。这个配合着Quick-R足够了。然后就配合着我这本专栏。有什么问题，特别欢迎留言，我收到后会跟大家有一个充分的交流和答疑。

这里做一个小小的广告，因为想学习交互D3的数据可视化，我自己用jekyll在github上托管了一个轻量级博客。域名是:[yangchao.me](yangchao.me)

上面会介绍一些我平时技术上遇到问题的总结，自己写的ggplot的简单教程等等，可能工具和内容更加丰富但是更加散漫。大家有兴趣可以去看下。有同学能注意到，我在这面写过了一些关于R的博客，所有东西是不会出现在这个专栏的，因为我不是那种偷懒的人。

我发现自己平时看豆瓣阅读都是在晚上上床睡觉之前，所以这样动手的技术博客可能本身并不适合。为了达到可以和大家更好的交流，我希望大家踊跃提出自己平时数据分析遇到的问题，我们来一个充分的互动。这样效果也许是最好的。

如果你现在连问题也都还提不出来，处于瞎逛的状态，我来给你提个问题。大家都来看看怎么弄，谁的代码最简洁，最方便。

现在数据是这样的。

<pre>
#    id       date      value
# 1: 27 2014-01-09 0.20597457
# 2: 27 2014-01-26 0.62911404
# 3: 27 2014-02-07 0.68702285
# 4: 37 2014-02-06 0.17655675
# 5: 37 2014-02-09 0.06178627
# 6: 37 2014-02-13 0.38410372
</pre>


有两个ID，三个日期。我先算出来对于每个ID，在最靠近的两个日期中，value的平均值。

实际的数据有500万行，id有3000个，日期400多天。所以用excel估计是够呛了，其他的方法大家可以想一下~
好了，这个就是这一期的作业。

不会不要紧，下一节我们就要讲啦~











