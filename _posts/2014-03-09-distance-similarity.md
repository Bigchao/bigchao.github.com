---
layout: post
categories: ml
title: 相似度和距离小结
---

在机器学习中，相似性度量或者距离函数对于像聚类，邻域搜索这样的算法是非常重要的。在做分类时常常需要估算不同样本之间的相似性度量(Similarity Measurement)，这时通常采用的方法就是计算样本间的“距离”(Distance)。采用什么样的方法计算距离是很讲究，甚至关系到分类的正确与否。



一般而言，定义一个距离函数d(x,y),需要满足以下几个准则：

1.  \$$d(x,y) = 0 $$;//到自己的距离为0 
2.  \$$d(x,y)>=0$$ // 距离要非负 
3.  对称性，\$$d(x,y) = d(y,x)$$ //如果A到B距离是a，那么B到A的距离也应该是a 
4.  三角形法则(两个之和大于第三边） \$$d(x,k)+ d(k,y) >= d(x,y)$$





**1 欧氏距离(Euclidean Distance)**

欧氏距离是最易于理解的一种距离计算方法，源自欧氏空间中两点间的距离公式。这也是我们平时学习用到最多的。

我们一般用到的是二范数的形式。

$$
D(x,y) = (\sum_{i=1}^{n}\left|x_i-y_i \right|)^{\frac{1}{2}}
$$

看起来是不是又熟悉又简单。

我们熟悉的欧氏距离虽然很有用，但也有明显的缺点。它将样品的不同属性（即各指标或各变量）之间的差别等同看待，这一点有时不能满足实际要求。

**2 曼哈顿距离(Manhattan Distance)**

**3 切比雪夫距离 ( Chebyshev Distance )**

**4 闵可夫斯基距离(Minkowski Distance)**

其实之所以把这些距离放在一起是因为这都差不多，都算是闵氏距离具体实例。

闵氏距离的定义：

两个n维变量\$$X(x_1,x_2,…,x_n)$$与 \$$Y(y_1,y_2,…,y_n)$$间的闵可夫斯基距离定义为：

$$
D(x,y) = ({\sum_{i=1}^{n}{\left|x_i-y_i \right|^p})^\frac{1}{p}
$$

其中\$$p$$是一个变参数。

根据变参数的不同，闵氏距离可以表示一类的距离。

当\$$p=1$$时，就是曼哈顿距离

当\$$p=2$$时，就是欧氏距离

当\$$p\to\infty$$时，就是切比雪夫距离

有关曼哈顿和切比雪夫距离的定义大家可以上网自己去查一下。
简单说来，闵氏距离的缺点主要有两个：(1)将各个分量的量纲(scale)，也就是“单位”当作相同的看待了。(2)没有考虑各个分量的分布（期望，方差等)可能是不同的。
       
**5 马氏距离(Mahalanobis Distance)**

马氏距离是由印度统计学家马哈拉诺比斯 (P. C. Mahalanobis)提出的，表示数据的协方差距离。它是一种有效的计算两个未知样本集的相似度的方法。与欧式距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的），并且是尺度无关的 (scale-invariant)，即独立于测量尺度。

有\$$M$$个样本向量\$$X_1~X_m$$，协方差矩阵记为\$$S$$，均值记为向量\$$\mu$$，则其中样本向量\$$X$$到\$$\mu$$的马氏距离表示为：




$$
Sample_1 = \{3,4,5,6\}

Sample_2 = \{2,2,8,4\}
$$

它们的均值为：
$$
\mu = \{2.5, 3, 6.5, 5\}
$$

协方差矩阵为：
$$
\begin{bmatrix}
0.25 & 0.50 & -0.75 & 0.50 \\
0.50 & 1.00 & -1.50 & 1.00 \\
-0.75 & -1.50 & 2.25 & -1.50 \\
0.50 & 1.00 & -1.50 & 1.00 
\end{bmatrix}
$$

然后套用我们刚才给出的公式就可以了。

需要注意的是：

1. 马氏距离的计算是建立在总体样本的基础上的，这一点从上述协方差矩阵的解释中可以得出，也就是说，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；
2. 计算马氏距离过程中，要求总体样本数大于样本的维数，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离来代替马氏距离，也可以理解为，如果样本数小于样本的维数，这种情况下求其中两个样本的距离，采用欧式距离计算即可。
3. 还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如 \$$A（3，4）$$， \$$B（ 5， 6）$$； \$$C（ 7， 8）$$，这种情况是 因为这三个样本在其所处的二维空间平面内共线（如果是大于二维的话，比较复杂？？？）。这种情况下，也采用欧式距离计算。
4. 在实际应用中 “总体样本数大于样本的维数 ”这个条件是很容易满足的，而所有样本点出现 3）中所描述的情况是很少出现的，所以在绝大多数情况下，马氏距 离是可以顺利计算的，但是马氏距离的计算是不稳定的，不稳定的来源是协方差矩阵，这也是马氏距离与欧式距离的最大差异之处。


dist(x,method="euclidean")即为计算欧式距离，其余可选的参数还有"maximum", "manhattan", "canberra", "binary" ,"minkowski"

参考链接
---
1. [机器学习中的相似性度量](http://blog.csdn.net/beta2/article/details/5045020)
2. [距离和相似性度量](http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html)
3. [距离、相似度和熵的度量方法总结](http://blog.csdn.net/v_july_v/article/details/8203674/)